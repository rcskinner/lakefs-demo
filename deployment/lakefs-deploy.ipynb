{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `LakeFS` to Drive Value in Development\n",
    "\n",
    "LakeFS is an essential tool for modern day development teams who are working with data lakes (S3, Azure Data Lake). LakeFS provides version control, backup, and workflow management soulutions that allow technical teams to: \n",
    "- `Experiment`: Safely experiment with copies of the production data lake without risking data lake contaimination \n",
    "- `Collaborate`:  Collaborate with other engineering teams on the development of engineering workflows\n",
    "\n",
    "When working with a data lake without LakeFS engineering teams have the tough choice of: \n",
    "- Slow down development by prohibiting in-situ experimentation and testing with production data\n",
    "- Digitally copy Data Lake data multiplying storage and hosting costs \n",
    "- Risk contaimination of the Datalake resulting in expensive rollback procedure, loss of newly generated data, & duplication of storage\n",
    "\n",
    "![Image](https://lakefs.io/wp-content/uploads/2022/03/Share-image_1200x630-2.png)\n",
    "\n",
    "LakeFS provides a highly scalable format agnostic zero copy operations that allow developers and engineering teams to manage their data like code. This demonstration will cover the following topics:\n",
    "\n",
    "1. Configuration of the LakeFS Client / Overview of the LakeFS Admin UI \n",
    "2. Initializing repositories and creating new branches \n",
    "3. Adding data to branches \n",
    "    - Adding data, committing \n",
    "    - Version differencing\n",
    "    - Merge operations\n",
    "4. Data Ops Cycle with LakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configure the LakeFS Client and Connect\n",
    "----\n",
    "In this section we'll demonstrate using the Python LakeFS API (`lakefs_client`) to interface with the LakeFS deployment. We'll instantiate an instance of the `LakeFSClient` object that allows us to communicate with and manipulate the state of the LakeFS instance using Python\n",
    "\n",
    "For this demo we will be primarily using the Python interface but LakeFS has developed Sofware Development Kits (SDKs) for: \n",
    "- Python\n",
    "- Java \n",
    "- goLang\n",
    "\n",
    "These SDKs allow developers to programatically access and integrate with LakeFS frictionlessly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 'https://cosmic-bat.lakefs-demo.io'\n",
    "USERNAME = \"AKIAJP5F7GBGE7V6OKZQ\"\n",
    "PASSWORD = \"AqInl1Ugb9tIAVVMHEQIKYaW0Feo3XhF7xiy4kgj\"\n",
    "REPO_NAME = 'demo-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and change working directory\n",
    "%cd \"C:\\Users\\rskin\\lakefs-demo\"\n",
    "import os\n",
    "from pathlib import Path, PurePosixPath\n",
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LakeFS client to connect to the service\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.host = HOST\n",
    "configuration.username = USERNAME\n",
    "configuration.password = PASSWORD\n",
    "client = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initalize a new Repository and create a new branch\n",
    "----\n",
    "\n",
    "In this section we'll create a new repository `stock-data`. We'll then create a branch called `data-upload` that we'll use to load our first set of Exchange Traded Fund (ETF) data. This section will cover the following concepts: \n",
    "- Initializing a new repository\n",
    "- Creating a new branch \n",
    "- Creating a protected branch \n",
    "\n",
    "\n",
    "Branches are used to create **isolated environments to perform data upload / experimentation**. This allows development teams to safely ingest data and test for data quality before releasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a repository \n",
    "repo = models.RepositoryCreation(\n",
    "    name= REPO_NAME, \n",
    "    storage_namespace='s3://treeverse-demo-lakefs-storage-production/user_cosmic-bat/demo-data', \n",
    "    default_branch='main')\n",
    "client.repositories.create_repository(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Creating a new branch from the latest commit (main) named data-upload. Creating a new branch allows developers/data engineers to \n",
    "easily track changes between branches. \n",
    "\"\"\"\n",
    "client.branches.create_branch(\n",
    "    repository=REPO_NAME, \n",
    "    branch_creation=models.BranchCreation(name='data-upload', source='main')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adding Data to branches\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "We've create two different helper functions to `upload_data` and `upload_dir` which will upload the contents of a single file or directory respectively. These two functions will be used to upload all of the ETF data inside of our `stock-data` directory. \n",
    "\n",
    "Once the data is uploaded we'll verify that the data has been loaded to the branch, check uncommited changes to verify we've uploaded the data we want, and commit the change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(branch:str,fname:str, lfs_client:LakeFSClient = client, repository:str = REPO_NAME):\n",
    "    \"\"\"Add data to the specified LakeFS Repositry / Branch\"\"\"\n",
    "    with open(fname, 'r', encoding='utf8', errors='ignore') as f:\n",
    "        client.objects.upload_object(repository=repository, branch=branch, path=fname,content=f)\n",
    "\n",
    "def broken_upload_dir(directory:str, branch:str,  repository:str = REPO_NAME):\n",
    "    \"\"\"Upload all files in a directory to LakeFS \"\"\"\n",
    "    directory = Path(directory)\n",
    "    dummy_counter = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if dummy_counter > 20: \n",
    "            break\n",
    "        path = os.path.join(directory / filename)\n",
    "        path = str(path)\n",
    "        upload_data(branch='data-upload', fname= path)\n",
    "        dummy_counter += 1\n",
    "\n",
    "\n",
    "# Upload data with a broken path\n",
    "broken_upload_dir('stock-data/ETFs', 'data-upload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_upload_dir(directory:str, branch:str,  repository:str = REPO_NAME):\n",
    "    \"\"\"Upload all files in a directory to LakeFS \"\"\"\n",
    "    directory = Path(directory)\n",
    "    dummy_counter = 0 \n",
    "    for filename in os.listdir(directory):\n",
    "        if dummy_counter > 20: \n",
    "            break\n",
    "        path = PurePosixPath(directory / filename)\n",
    "        path = str(path)\n",
    "        upload_data(branch='data-upload', fname= path)\n",
    "        dummy_counter += 1 \n",
    "\n",
    "fixed_upload_dir('stock-data/ETFs', 'data-upload')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing Development Cycle Using LakeFS\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a Exchange Traded Fund data loaded we'll have our development teams open a new branch to add the stock data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'admin',\n",
       " 'creation_date': 1650285007,\n",
       " 'id': '4a9a8ffacae9c82597139e444ff8e2ca21e8909bf828458d74911b4203ac1663',\n",
       " 'message': 'Added stock data',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'type': 'data-upload'},\n",
       " 'parents': ['71d3322a76cb0d7a2124c2772dcb29b93d64d69a0b210c86514b42efc1e4bf04']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploadging the remaining stock data\n",
    "fixed_upload_dir('stock-data/Stocks','data-upload')\n",
    "\n",
    "# Programatically commiting and merging the branch \n",
    "client.commits.commit(\n",
    "    repository = REPO_NAME,\n",
    "    branch='data-upload',\n",
    "    commit_creation={\n",
    "        \"message\":\"Added stock data\", \n",
    "        \"metadata\":{\n",
    "            'type':'data-upload'\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Object Storage meta-data\n",
    "\n",
    "The LakeFS client allows you to access the object's metadata layer without directly accessing the storage layer (resulting in a significant reduction in storage costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('production-metadata.json', 'w') as f:\n",
    "    metadata = client.objects.list_objects('demo-data', '71d3322a76cb0d7a2124c2772dcb29b93d64d69a0b210c86514b42efc1e4bf04')\n",
    "    json.dump(metadata.to_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating S3 API Consistency\n",
    "----\n",
    "\n",
    "LakeFS has been developed from the ground up to us the S3 API to access the meta-data and storage layers allowing developers to easily implement LakeFS to manage data versioning and development workflows with little to no engineering change. \n",
    "\n",
    "![LakeFS Architecture](https://docs.lakefs.io/assets/img/arch.png)\n",
    "\n",
    "In this section we'll demonstrate using the most Python S3 interface (`boto3`) to programatically access the object storage and metadata layers of our LakeFS storage. This will allow teams to access, manipulate, and download storage using a common API, only changing the target buckets and authentication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client(\n",
    "    's3', \n",
    "    endpoint_url = HOST, \n",
    "    aws_access_key_id = USERNAME, \n",
    "    aws_secret_access_key = PASSWORD\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Key': 'initial-upload/stock-data/ETFs/aadr.us.txt',\n",
       " 'LastModified': datetime.datetime(2022, 4, 18, 12, 18, 41, 970000, tzinfo=tzutc()),\n",
       " 'ETag': '\"dfcd8314aabce0eadc2362b663c4327d\"',\n",
       " 'Size': 70908,\n",
       " 'StorageClass': 'STANDARD'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recover the latest \n",
    "list_resp = s3.list_objects(Bucket=REPO_NAME, Prefix='initial-upload/')\n",
    "list_resp['Contents'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering from Production Data Loss\n",
    "\n",
    "LakeFS is an essential tool in preventing data loss within Data Lakes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_production():\n",
    "    \"\"\"Delete all the data on the main branch, creating a loss of production data situation\"\"\" \n",
    "    objects = s3.list_objects(Bucket=REPO_NAME, Prefix='main/')\n",
    "    for obj in objects['Contents']: \n",
    "        s3.delete_object(Bucket=REPO_NAME, Key=obj['Key'])\n",
    "\n",
    "\n",
    "\n",
    "def really_delete_production():\n",
    "    \"\"\"Delete all the data on the main (production) branch and commit the change\"\"\"\n",
    "    objects = s3.list_objects(Bucket=REPO_NAME, Prefix='main/')\n",
    "    for obj in objects['Contents']: \n",
    "        s3.delete_object(Bucket=REPO_NAME, Key=obj['Key'])\n",
    "    \n",
    "    client.commits.commit(\n",
    "        repository=REPO_NAME, \n",
    "        branch='main',\n",
    "        commit_creation={'message':\"Removing data from the production branch\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_production()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "really_delete_production()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.model.revert_creation import RevertCreation\n",
    "\n",
    "revert_creation = RevertCreation(\n",
    "    ref='15a4fe006a4bb6612f8d05b7e6e0e9377444a8e47337b373efc92f302f025557',\n",
    "    parent_number=1\n",
    "    )\n",
    "\n",
    "client.branches.revert_branch(\n",
    "    repository=REPO_NAME, \n",
    "    branch='main', \n",
    "    revert_creation=revert_creation\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a75908cf41ea5be0db61fa6abf8b7a151ad4355677d7ee25492eb64cb967730"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('lakefs-demo-utJ5wb3e-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
