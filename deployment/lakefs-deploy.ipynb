{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `LakeFS` to Drive Value in Development\n",
    "\n",
    "LakeFS is an essential tool for modern day development teams who are working with data lakes (S3, Azure Data Lake). LakeFS provides version control, backup, and workflow management soulutions that allow technical teams to: \n",
    "- `Experiment`: Safely experiment with copies of the production data lake without risking data lake contaimination \n",
    "- `Collaborate`:  Collaborate with other engineering teams on the development of engineering workflows\n",
    "\n",
    "When working with a data lake without LakeFS engineering teams have the tough choice of: \n",
    "- Slow down development by prohibiting in-situ experimentation and testing with production data\n",
    "- Digitally copy Data Lake data multiplying storage and hosting costs \n",
    "- Risk contaimination of the Datalake resulting in expensive rollback procedure, loss of newly generated data, & duplication of storage\n",
    "\n",
    "![Image](https://lakefs.io/wp-content/uploads/2022/03/Share-image_1200x630-2.png)\n",
    "\n",
    "LakeFS provides a highly scalable format agnostic zero copy operations that allow developers and engineering teams to manage their data like code. This demonstration will cover the following topics:\n",
    "\n",
    "1. Configuration of the LakeFS Client / Overview of the LakeFS Admin UI \n",
    "2. Initializing repositories and creating new branches \n",
    "3. Adding data to branches \n",
    "    - Adding data, committing \n",
    "    - Version differencing\n",
    "    - Merge operations\n",
    "4. Data Ops Cycle with LakeFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configure the LakeFS Client and Connect\n",
    "----\n",
    "In this section we'll demonstrate using the Python LakeFS API (`lakefs_client`) to interface with the LakeFS deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and change working directory\n",
    "%cd \"C:\\Users\\rskin\\lakefs-demo\"\n",
    "import os\n",
    "from pathlib import Path, PurePosixPath\n",
    "\n",
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LakeFS client to connect to the service\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.host = 'https://adapted-husky.lakefs-demo.io'\n",
    "configuration.username = \"AKIAJRRULG47CCC5Q6QQ\"\n",
    "configuration.password = \"JSHymZ3sCBwhrHQlJCq4rYDKC/MY4/NzExoVaLdH\"\n",
    "client = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initalize a new Repository and create a new branch\n",
    "----\n",
    "\n",
    "In this section we'll create a new repository `stock-data`. We'll then create a branch called `data-upload` that we'll use to load our first set of Exchange Traded Fund (ETF) data. This section will cover the following concepts: \n",
    "- Initializing a new repository\n",
    "- Creating a new branch \n",
    "- Creating a protected branch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a repository \n",
    "name = 'stock-data'\n",
    "repo = models.RepositoryCreation(\n",
    "    name= name, \n",
    "    storage_namespace='s3://treeverse-demo-lakefs-storage-production/user_adapted-husky/stock-data', \n",
    "    default_branch='main')\n",
    "client.repositories.create_repository(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Creating a new branch from the latest commit (main) named data-upload. Creating a new branch allows developers/data engineers to \n",
    "easily track changes between branches. \n",
    "\"\"\"\n",
    "client.branches.create_branch(\n",
    "    repository='stock-data', \n",
    "    branch_creation=models.BranchCreation(name='data-upload', source='main')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a protected branch via UI\n",
    "\n",
    "![demo](deploy-photos\\protected-branch1.png)\n",
    "![demo](deploy-photos\\protected-branch2.png)\n",
    "![demo](deploy-photos\\protected-branch3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adding Data to branches\n",
    "\n",
    "----\n",
    "\n",
    "We've create two different helper functions to `upload_data` and `upload_dir` which will upload the contents of a single file or directory respectively. These two functions will be used to upload all of the ETF data inside of our `stock-data` directory. \n",
    "\n",
    "Once the data is uploaded we'll verify that the data has been loaded to the branch, check uncommited changes to verify we've uploaded the data we want, and commit the change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = 'stock-data'\n",
    "def upload_data(branch:str,fname:str, lfs_client:LakeFSClient = client, repository:str = repo_name):\n",
    "    \"\"\"Add data to the specified LakeFS Repositry / Branch\"\"\"\n",
    "    with open(fname, 'r', encoding='utf8', errors='ignore') as f:\n",
    "        client.objects.upload_object(repository=repository, branch=branch, path=fname,content=f)\n",
    "\n",
    "def upload_dir(directory:str, branch:str,  repository:str = repo_name):\n",
    "    \"\"\"Upload all files in a directory to LakeFS \"\"\"\n",
    "    directory = Path(directory)\n",
    "    for filename in os.listdir(directory):\n",
    "        path = PurePosixPath(directory / filename)\n",
    "        path = str(path)\n",
    "        upload_data(branch='data-upload', fname= path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_dir('stock-data/ETFs', 'data-upload')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is uploaded we'll merge the `upload-data` branch into Main. We'll expect to see an error (since our `Main` branch is one of the protected branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(branch='main', fname='stock-data/stocks/aamc.us.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Managing Development Cycle Using LakeFS\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a Exchange Traded Fund data loaded we'll have our development teams open a new branch to add the stock data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a75908cf41ea5be0db61fa6abf8b7a151ad4355677d7ee25492eb64cb967730"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('lakefs-demo-utJ5wb3e-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
