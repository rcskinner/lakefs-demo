{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `LakeFS` to Drive Value in Development\n",
    "\n",
    "LakeFS is an essential tool for modern day development teams who are working with data lakes (S3, Azure Data Lake). LakeFS provides version control, backup, and workflow management soulutions that allow technical teams to: \n",
    "- `Experiment`: Safely experiment with copies of the production data lake without risking data lake contaimination \n",
    "- `Collaborate`:  Collaborate with other engineering teams on the development of engineering workflows\n",
    "\n",
    "When working with a data lake without LakeFS engineering teams have the tough choice of: \n",
    "- Slow down development by prohibiting in-situ experimentation and testing with production data\n",
    "- Digitally copy Data Lake data multiplying storage and hosting costs \n",
    "- Risk contaimination of the Datalake resulting in expensive rollback procedure, loss of newly generated data, & duplication of storage\n",
    "\n",
    "![Image](https://lakefs.io/wp-content/uploads/2022/03/Share-image_1200x630-2.png)\n",
    "\n",
    "LakeFS provides a highly scalable format agnostic zero copy operations that allow developers and engineering teams to manage their data like code. This demonstration will cover the following topics:\n",
    "\n",
    "1. Configuration of the LakeFS Client / Overview of the LakeFS Admin UI \n",
    "2. Initializing repositories and creating new branches \n",
    "3. Adding data to branches \n",
    "    - Adding data, committing \n",
    "    - Version differencing\n",
    "    - Merge operations\n",
    "    - Version Tagging\n",
    "4. Data Ops Cycle with LakeFS\n",
    "5. Recovering from Production Data Loss \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configure the LakeFS Client and Connect\n",
    "----\n",
    "In this section we'll demonstrate using the Python LakeFS API (`lakefs_client`) to interface with the LakeFS deployment. We'll instantiate an instance of the `LakeFSClient` object that allows us to communicate with and manipulate the state of the LakeFS instance using Python\n",
    "\n",
    "For this demo we will be primarily using the Python interface but LakeFS has developed Sofware Development Kits (SDKs) for: \n",
    "- Python\n",
    "- Java \n",
    "- goLang\n",
    "\n",
    "These SDKs allow developers to programatically access and integrate with LakeFS frictionlessly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = 'https://cosmic-bat.lakefs-demo.io'\n",
    "USERNAME = \"AKIAJP5F7GBGE7V6OKZQ\"\n",
    "PASSWORD = \"AqInl1Ugb9tIAVVMHEQIKYaW0Feo3XhF7xiy4kgj\"\n",
    "REPO_NAME = 'demo-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rskin\\lakefs-demo\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries and change working directory\n",
    "%cd \"C:\\Users\\rskin\\lakefs-demo\"\n",
    "import os\n",
    "from pathlib import Path, PurePosixPath\n",
    "import lakefs_client\n",
    "from lakefs_client import models\n",
    "from lakefs_client.client import LakeFSClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LakeFS client to connect to the service\n",
    "configuration = lakefs_client.Configuration()\n",
    "configuration.host = HOST\n",
    "configuration.username = USERNAME\n",
    "configuration.password = PASSWORD\n",
    "client = LakeFSClient(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initalize a new Repository and create a new branch\n",
    "----\n",
    "\n",
    "In this section we'll create a new repository `stock-data`. We'll then create a branch called `data-upload` that we'll use to load our first set of Exchange Traded Fund (ETF) data. This section will cover the following concepts: \n",
    "- Initializing a new repository\n",
    "- Creating a new branch \n",
    "- Creating a protected branch \n",
    "\n",
    "\n",
    "Branches are used to create **isolated environments to perform data upload / experimentation**. This allows development teams to safely ingest data and test for data quality before releasing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'creation_date': 1650366765,\n",
       " 'default_branch': 'main',\n",
       " 'id': 'demo-data',\n",
       " 'storage_namespace': 's3://treeverse-demo-lakefs-storage-production/user_cosmic-bat/demo-data'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a repository \n",
    "repo = models.RepositoryCreation(\n",
    "    name= REPO_NAME, \n",
    "    storage_namespace='s3://treeverse-demo-lakefs-storage-production/user_cosmic-bat/demo-data', \n",
    "    default_branch='main')\n",
    "client.repositories.create_repository(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'57d8bb999de708da4b3a3be049496b246ed6d96c68e516343fe2bce9923a3c7c'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Creating a new branch from the latest commit (main) named data-upload. Creating a new branch allows developers/data engineers to \n",
    "easily track changes between branches. \n",
    "\"\"\"\n",
    "client.branches.create_branch(\n",
    "    repository=REPO_NAME, \n",
    "    branch_creation=models.BranchCreation(name='data-upload', source='main')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adding Data to branches\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "We've create two different helper functions to `upload_data` and `upload_dir` which will upload the contents of a single file or directory respectively. These two functions will be used to upload all of the ETF data inside of our `stock-data` directory. \n",
    "\n",
    "Once the data is uploaded we'll verify that the data has been loaded to the branch, check uncommited changes to verify we've uploaded the data we want, and commit the change\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(branch:str,fname:str, lfs_client:LakeFSClient = client, repository:str = REPO_NAME):\n",
    "    \"\"\"Add data to the specified LakeFS Repositry / Branch\"\"\"\n",
    "    with open(fname, 'r', encoding='utf8', errors='ignore') as f:\n",
    "        client.objects.upload_object(repository=repository, branch=branch, path=fname,content=f)\n",
    "\n",
    "def broken_upload_dir(directory:str, branch:str,  repository:str = REPO_NAME):\n",
    "    \"\"\"Upload all files in a directory to LakeFS \"\"\"\n",
    "    directory = Path(directory)\n",
    "    dummy_counter = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if dummy_counter > 20: \n",
    "            break\n",
    "        path = os.path.join(directory / filename)\n",
    "        path = str(path)\n",
    "        upload_data(branch='data-upload', fname= path)\n",
    "        dummy_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data with a broken path\n",
    "broken_upload_dir('stock-data/ETFs', 'data-upload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_upload_dir(directory:str, branch:str,  repository:str = REPO_NAME):\n",
    "    \"\"\"Upload all files in a directory to LakeFS \"\"\"\n",
    "    directory = Path(directory)\n",
    "    dummy_counter = 0 \n",
    "    for filename in os.listdir(directory):\n",
    "        if dummy_counter > 20: \n",
    "            break\n",
    "        path = PurePosixPath(directory / filename)\n",
    "        path = str(path)\n",
    "        upload_data(branch='data-upload', fname= path)\n",
    "        dummy_counter += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_upload_dir('stock-data/ETFs', 'data-upload')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a Exchange Traded Fund data loaded we'll have our development teams add the Stock data to the `data-upload` branch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'committer': 'admin',\n",
       " 'creation_date': 1650366812,\n",
       " 'id': '6a650f0166480c10faac7bb828b65a08dfccaed75337e160d3442d1db97a6720',\n",
       " 'message': 'Added stock data',\n",
       " 'meta_range_id': '',\n",
       " 'metadata': {'type': 'data-upload'},\n",
       " 'parents': ['9a9b15e690d8457ef3653fe54dfe64c197a77dfac013f97e7582ed642b6821fa']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploadging the remaining stock data\n",
    "fixed_upload_dir('stock-data/Stocks','data-upload')\n",
    "\n",
    "# Programatically commiting and merging the branch \n",
    "client.commits.commit(\n",
    "    repository = REPO_NAME,\n",
    "    branch='data-upload',\n",
    "    commit_creation={\n",
    "        \"message\":\"Added stock data\", \n",
    "        \"metadata\":{\n",
    "            'type':'data-upload'\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've loaded the ETF data to our `data-upload` branch and also added the Stock data to our `data-upload` branch, its now time to merge the `data-upload` branch into our production branch `main`. \n",
    "\n",
    "By adding the data to the `data-upload` branch we can ensure that the production release is updated atomically, simplifying the data lineage, ensuring that all the data is added, and protecting the main production release from only having half the data loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Demonstrating S3 API Consistency\n",
    "----\n",
    "\n",
    "In this section we'll demonstrate using the most Python S3 interface (`boto3`) to programatically access the object storage and metadata layers of our LakeFS storage. This will allow teams to access, manipulate, and download storage using a common API, only changing the target buckets and authentication. LakeFS has been designed to work with the S3 interfaces, allowing engineering and end-user teams to work with familiar APIs without re-engineering data uploading / data connection processes respectively. Other applications that use the S3 API include but aren't limited to: \n",
    " - Spark \n",
    " - Kafka \n",
    " - Hadoop\n",
    " - Hudi\n",
    "\n",
    "\n",
    "![LakeFS Architecture](https://docs.lakefs.io/assets/img/arch.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client(\n",
    "    's3', \n",
    "    endpoint_url = HOST, \n",
    "    aws_access_key_id = USERNAME, \n",
    "    aws_secret_access_key = PASSWORD\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Key': 'main/stock-data/ETFs/aadr.us.txt',\n",
       " 'LastModified': datetime.datetime(2022, 4, 19, 11, 12, 55, 639000, tzinfo=tzutc()),\n",
       " 'ETag': '\"dfcd8314aabce0eadc2362b663c4327d\"',\n",
       " 'Size': 70908,\n",
       " 'StorageClass': 'STANDARD'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recover the latest meta data for the main branch\n",
    "list_resp = s3.list_objects(Bucket=REPO_NAME, Prefix='main/')\n",
    "list_resp['Contents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Key': 'latest-tag/stock-data/ETFs/aadr.us.txt',\n",
       " 'LastModified': datetime.datetime(2022, 4, 19, 11, 12, 55, 639000, tzinfo=tzutc()),\n",
       " 'ETag': '\"dfcd8314aabce0eadc2362b663c4327d\"',\n",
       " 'Size': 70908,\n",
       " 'StorageClass': 'STANDARD'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_resp = s3.list_objects(Bucket=REPO_NAME, Prefix='latest-tag/')\n",
    "list_resp['Contents'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Recovering from Production Data Loss\n",
    "----\n",
    "\n",
    "LakeFS is an essential tool in preventing data loss within Data Lakes. In this section we'll demonstrate the following: \n",
    " - Recovering deletion of production of data by reverting an uncommited change\n",
    " - Recovering production data status of a commited change by reverting to the last stable commit\n",
    " - Preventing the loss of production data by setting up a protected branch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_production():\n",
    "    \"\"\"Delete all the data on the main branch, creating a loss of production data situation\"\"\" \n",
    "    objects = s3.list_objects(Bucket=REPO_NAME, Prefix='main/')\n",
    "    for obj in objects['Contents']: \n",
    "        s3.delete_object(Bucket=REPO_NAME, Key=obj['Key'])\n",
    "\n",
    "\n",
    "\n",
    "def really_delete_production():\n",
    "    \"\"\"Delete all the data on the main (production) branch and commit the change\"\"\"\n",
    "    objects = s3.list_objects(Bucket=REPO_NAME, Prefix='main/')\n",
    "    for obj in objects['Contents']: \n",
    "        s3.delete_object(Bucket=REPO_NAME, Key=obj['Key'])\n",
    "    \n",
    "    client.commits.commit(\n",
    "        repository=REPO_NAME, \n",
    "        branch='main',\n",
    "        commit_creation={'message':\"Removing data from the production branch\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the production environment without committing. \n",
    "# We'll then use the LakeFS UI to revert the uncommitted changes\n",
    "delete_production()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete and commit, replicating encountering a commit without data \n",
    "really_delete_production()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverting the previous commit to restore production data with no-copy\n",
    "from lakefs_client.model.revert_creation import RevertCreation\n",
    "revert_creation = RevertCreation(\n",
    "    ref='0e7223c8f343112d1a242b5674bb82ceb891b9875db4fabac7fe358e7c0ec4ea',\n",
    "    parent_number=1\n",
    "    )\n",
    "\n",
    "client.branches.revert_branch(\n",
    "    repository=REPO_NAME, \n",
    "    branch='main', \n",
    "    revert_creation=revert_creation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without LakeFS's branching and commit system we would have the following options: \n",
    "- Rerun all pipelines / implement major engineering effort to ensure data lake is restored successfully \n",
    "- Significant recovery cost from recovery S3 buckets with extended recovery time\n",
    "    - Also increased storage cost for duplicated data \n",
    "- Accept the data loss\n",
    "\n",
    "LakeFS' commit and branching system allow the engineering teams to effortlessly recover from disastarous production data loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preventing the loss of Production Data \n",
    "\n",
    "To prevent disruption to the business, we can define protected branches using LakeFS. A protected branch cannot be written into / deleted from directly, and can only be edited via a merge request. This allows the development teams to create protected branches that cannot be edited by end-users, preserving the integrity of the DataLake.\n",
    "\n",
    "No other service allows the explicit protection of data lake objects without redundant duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lakefs_client.model.branch_protection_rule import BranchProtectionRule\n",
    "\n",
    "branch_protection_rule = BranchProtectionRule(pattern=\"main\")\n",
    "client.repositories.create_branch_protection_rule(\n",
    "    repository=REPO_NAME,\n",
    "    branch_protection_rule=branch_protection_rule\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll attempt to run our `really_delete_production` function, but will be blocked by the branch protection rule we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encountered an error attempting to delete production data due to protected branch\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    really_delete_production()\n",
    "except Exception as e: \n",
    "    print(\"encountered an error attempting to delete production data due to protected branch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Replicate DataLake at time of Error\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One key feature of LakeFS is its ability to allow teams to \"time travel\" through the data lake, and access what the state of the datalake was at a given point. This is critical in allowing development teams to isolate and stabilize a particular branch while hunting a bug. \n",
    "\n",
    "In this example we'll demonstrate pulling data from specific commit reference IDs and show how LakeFS enables users to quickly get a copy of what the state of the datalake was at a given time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def load_data_at_ref(ref:str):\n",
    "    try: \n",
    "        shutil.rmtree('C:/Users/rskin/lakefs-demo/isolated-environment')\n",
    "    except Exception: \n",
    "        pass\n",
    "    ref = ref + '/'\n",
    "    objects = s3.list_objects(Bucket=REPO_NAME, Prefix=ref)\n",
    "    for obj in objects['Contents']: \n",
    "        fname = Path('isolated-environment',obj['Key'])\n",
    "        dir = os.path.dirname(fname)\n",
    "        if not os.path.isdir(dir):\n",
    "            os.makedirs(os.path.abspath(dir))\n",
    "        with open(fname, 'wb') as f: \n",
    "            s3.download_fileobj(REPO_NAME,obj['Key'],  f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_at_ref('128160ce76f70c2797c178d2a0b0fcec41b56a92cbc44f7e50ca1b219cc5aa98')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature is a critical enabler in large data lakes, allowing engineers to test functionality, test issues with deployment pipelines, and experiment on the exact state of the data at a given commit. \n",
    "\n",
    "Without this functionality engineering teams would need to develop custom pipelines that would generate the expected state of the data (if even feasible, depending on the statefulness of the pipelines). With LakeFS getting a perfect copy of the data lake is a trivial operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting and Creating New Feature Branches\n",
    "\n",
    "In this section we'll demonstrate how data engineering teams can use LakeFS to test development directly on production data without risking damage to the production release. \n",
    "\n",
    "We'll be converting data from an inefficient format (.txt) into a more scalable, data-lake friendly format (.parquet)\n",
    "\n",
    "**Directly editing on the `/parquet-conversion` branch**\n",
    "![Directly Editing](deploy-photos\\directly-editing.drawio.png)\n",
    "\n",
    "We'll do this operation in place on an exactl replica of the production data by leveraging the LakeFS branching model and copy-on-write paradigms to create an exact duplicate of the data in `/main` without risking the production data or creating duplicated storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new client\n",
    "from lakefs_client.model.branch_creation import BranchCreation\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "branch_creation = BranchCreation(\n",
    "    name = 'parquet-conversion',\n",
    "    source='main'\n",
    ")\n",
    "try: \n",
    "    client.branches.create_branch(REPO_NAME, branch_creation)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_reformat(): \n",
    "    # List all bucket objs \n",
    "    bucket_objs = s3.list_objects(Bucket=REPO_NAME, Prefix='parquet-conversion/')\n",
    "\n",
    "    for obj in bucket_objs['Contents']: \n",
    "        key = obj['Key']\n",
    "        # Fetch the S3 / read the S3 response into filestream\n",
    "        s3_response = s3.get_object(Bucket=REPO_NAME, Key = key)\n",
    "        content = s3_response['Body'].read()\n",
    "        df = pd.read_csv(io.BytesIO(content))\n",
    "\n",
    "        # Put the new parquet file back\n",
    "        s3.put_object(Body=df.to_parquet(), Bucket=REPO_NAME, Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_reformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_parquet_reformat():\n",
    "    # List all bucket objs \n",
    "    bucket_objs = s3.list_objects(Bucket=REPO_NAME, Prefix='parquet-conversion/')\n",
    "\n",
    "    for obj in bucket_objs['Contents']: \n",
    "        key = obj['Key']\n",
    "        \n",
    "        # Fetch the S3 / read the S3 response into filestream\n",
    "        s3_response = s3.get_object(Bucket=REPO_NAME, Key = key)\n",
    "        content = s3_response['Body'].read()\n",
    "        df = pd.read_csv(io.BytesIO(content))\n",
    "        \n",
    "        # Generate new key name\n",
    "        basename = os.path.basename(key)\n",
    "        dirname = os.path.dirname(key)\n",
    "        basename = basename.replace('.txt', '.parquet')\n",
    "        parquet_key = PurePosixPath(dirname, basename)\n",
    "\n",
    "        # Put the new parquet file back\n",
    "        s3.put_object(\n",
    "            Body=df.to_parquet(), \n",
    "            Bucket=REPO_NAME, \n",
    "            Key=str(parquet_key))\n",
    "        \n",
    "        # Delete the old text files \n",
    "        s3.delete_object(Bucket= REPO_NAME, Key=key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parquet_reformat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 1: Duplicate the Object Storage\n",
    "Without the LakeFS branching models, we can directly develop and test our conversion function on Production data by duplicating the object storage and manipulating the duplicated object storage. \n",
    "\n",
    "![duplicating storage](deploy-photos\\duplicating.drawio.png)\n",
    "\n",
    "**Risks:** \n",
    "- Expensive \n",
    "    - Duplicated storage cost\n",
    "    - 3x read/write costs\n",
    "    - Expensive compute pipeline for large lakes\n",
    "- Time consuming for large lakes \n",
    "- Requires Data Engineering effort to implement for larger buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 2: Develop and Test using Representative Subset\n",
    "\n",
    "The second alternative is to develop and test the function (`parquet_conversion`) using a representative sample of the data-lake. This method involves sampling from the data lake and duplicating a small portion to test the function on before applying the function to the production dataset\n",
    "\n",
    "\n",
    "![Representitive Subset](deploy-photos\\representitive-subset.drawio.png)\n",
    "\n",
    "\n",
    "This alternative avoids the expensive copy and read/write operations by creating a small representative dataset. This method still presents significant development risks\n",
    "\n",
    "**Risks**\n",
    "- Development effort required to generate representative data \n",
    "- Risk missing important data structures / edge conditions with representation \n",
    "- At the end of the pipeline the function must directly edit the base datalake \n",
    "    - Increase V&V criticality / extensiveness of testing and development"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a75908cf41ea5be0db61fa6abf8b7a151ad4355677d7ee25492eb64cb967730"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('lakefs-demo-utJ5wb3e-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
